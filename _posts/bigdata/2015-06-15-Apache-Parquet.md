---
layout: post
---

### Create Hive Table with Parquet format

åˆ›å»ºhiveè¡¨, åŒ…æ‹¬å¤æ‚ç±»å‹. å­˜å‚¨æ ¼å¼ä¸ºparquet

```
hive> CREATE TABLE alltypesparquet (
c1 int,
c2 boolean,
c3 double,
c4 string,
c5 array<int>,
c6 map<int,string>,
c7 map<string,string>,
c8 struct<r:string,s:int,t:double>,
c9 tinyint,
c10 smallint,
c11 float,
c12 bigint,
c13 array<array<string>> ,
c15 struct<r:int,s:struct<a:int,b:string>> ,
c16 array<struct<m:map<string,string>,n:int>>
) STORED AS PARQUET
```

æŸ¥çœ‹è¡¨ç»“æ„

```
hive> desc alltypesparquet;
OK
c1                  	int
c2                  	boolean
c3                  	double
c4                  	string
c5                  	array<int>
c6                  	map<int,string>
c7                  	map<string,string>
c8                  	struct<r:string,s:int,t:double>
c9                  	tinyint
c10                 	smallint
c11                 	float
c12                 	bigint
c13                 	array<array<string>>
c15                 	struct<r:int,s:struct<a:int,b:string>>
c16                 	array<struct<m:map<string,string>,n:int>>
```

åŠ è½½æœ¬åœ°æ•°æ®åˆ°è¡¨ä¸­. æ•°æ®æ¥æºäº: hive_alltypes.parquet: <https://issues.apache.org/jira/browse/DRILL-2005>

```
hive> LOAD DATA LOCAL INPATH '/home/qihuang.zheng/hive_alltypes.parquet' OVERWRITE INTO TABLE alltypesparquet;
hive> select * from alltypesparquet;
1 true  1.1 1 [1,2] {1:"x",2:"y"} {"k":"v"} {"r":"a","s":9,"t":2.2} 1 1 1.0 1 [["a","b"],["c","d"]] {"r":1,"s":{"a":2,"b":"x"}} [{"m":null,"n":1},{"m":{"a":"b","c":"d"},"n":2}]
hive> select c6 from alltypesparquet;
{1:"x",2:"y"}
```

### Drill query Parquet

æ³¨æ„ä¸Šé¢c6,c7ä½œä¸ºMapçš„è¾“å‡º. åœ¨hiveä¸­æ˜¯$key:$value. $è¡¨ç¤ºç»“æœ.  ä½†æ˜¯åœ¨Drillä¸­çš„è¾“å‡º:  

```
0: jdbc:drill:zk=local> select * from  dfs.`/home/qihuang.zheng/hive_alltypes.parquet`;
+----+----+----+----+----+----+----+----+----+-----+-----+-----+-----+-----+-----+
| c1 | c2 | c3 | c4 | c5 | c6 | c7 | c8 | c9 | c10 | c11 | c12 | c13 | c15 | c16 |
+----+----+----+----+----+----+----+----+----+-----+-----+-----+-----+-----+-----+
| 1 | true | 1.1 | [B@282fd12a | {"bag":[{"array_element":1},{"array_element":2}]} | {"map":[{"key":1,"value":"eA=="},{"key":2,"value":"eQ=="}]} | {"map":[{"key":"aw==","value":"dg=="}]} | {"r":"YQ==","s":9,"t":2.2} | 1 | 1 | 1.0 | 1 | {"bag":[{"array_element":{"bag":[{"array_element":"YQ=="},{"array_element":"Yg=="}]}},{"array_element":{"bag":[{"array_element":"Yw=="},{"array_element":"ZA=="}]}}]} | {"r":1,"s":{"a":2,"b":"eA=="}} | {"bag":[{"array_element":{"m":{"map":[]},"n":1}},{"array_element":{"m":{"map":[{"key":"YQ==","value":"Yg=="},{"key":"Yw==","value":"ZA=="}]},"n":2}}]} |
+----+----+----+----+----+----+----+----+----+-----+-----+-----+-----+-----+-----+
```

å®ƒçš„Mapè¾“å‡º: `{"map":[{"key":1,"value":"eA=="},{"key":2,"value":"eQ=="}]}` è€Œä¸æ˜¯hiveçš„: `{1:"x",2:"y"}`.  
ä¸ä»…ä»…ç»“æ„å‘ç”Ÿäº†å˜åŒ–, è¿è¾“å‡ºçš„x,yéƒ½å˜æˆäº†eA==,eQ==. æ˜¾ç„¶Drillåœ¨å¤„ç†Parquetæ–‡ä»¶æ—¶æœ‰ç‚¹é—®é¢˜.  
1.mapçš„å€¼æ˜¯ä¸€ä¸ªæ•°ç»„äº†,å› æ­¤ä¸èƒ½ä½¿ç”¨t.c6.key,è€Œåº”è¯¥ä½¿ç”¨t.c6[0].key. ä½†æ˜¯å¦‚æœè¦æ‰€æœ‰çš„åˆ—å‘¢?  
2.ç”¨flattenç»“æ„,ä½†æ˜¯æœ€ç»ˆä¼šå°†ä¸€è¡Œå˜æˆå¤šè¡Œ.  

### Parquet-Tools

ä½¿ç”¨parquetè‡ªå¸¦çš„å·¥å…·çš„schemaå‘½ä»¤, å¯ä»¥ç›´æ¥æŸ¥çœ‹parquetæ–‡ä»¶çš„ç»“æ„:  

```
[qihuang.zheng@dp0653 ~] tar zxf parquet-1.5.0-cdh5.4.2.tar.gz && cd arquet-1.5.0-cdh5.4.2/parquet-tools/target && tar zxf parquet-tools-1.5.0-cdh5.4.2.tar.gz && cd parquet-tools-1.5.0-cdh5.4.2
[qihuang.zheng@dp0653 parquet-tools-1.5.0-cdh5.4.2]$ ./parquet-tools schema /home/qihuang.zheng/hive_alltypes.parquet
message hive_schema {
  optional int32 c1;
  optional boolean c2;
  optional double c3;
  optional binary c4;
  optional group c5 (LIST) {
    repeated group bag {
      optional int32 array_element;
    }
  }
  optional group c6 (MAP) {
    repeated group map (MAP_KEY_VALUE) {
      required int32 key;
      optional binary value;
    }
  }
  optional group c7 (MAP) {
    repeated group map (MAP_KEY_VALUE) {
      required binary key;
      optional binary value;
    }
  }
  optional group c8 {
    optional binary r;
    optional int32 s;
    optional double t;
  }
  optional int32 c9;
  optional int32 c10;
  optional float c11;
  optional int64 c12;
  optional group c13 (LIST) {
    repeated group bag {
      optional group array_element (LIST) {
        repeated group bag {
          optional binary array_element;
        }
      }
    }
  }
  optional group c15 {
    optional int32 r;
    optional group s {
      optional int32 a;
      optional binary b;
    }
  }
  optional group c16 (LIST) {
    repeated group bag {
      optional group array_element {
        optional group m (MAP) {
          repeated group map (MAP_KEY_VALUE) {
            required binary key;
            optional binary value;
          }
        }
        optional int32 n;
      }
    }
  }
}
```

### How-to: Use Parquet with Impala, Hive, Pig, and MapReduce
<http://blog.cloudera.com/blog/2014/03/how-to-use-parquet-with-impala-hive-pig-mapreduce/>  
<https://github.com/cloudera/parquet-examples/tree/master/MapReduce>

è¿è¡Œ```TestReadWriteParquet```(è¾“å…¥è·¯å¾„æŒ‡å®šä¸ºhive_alltypes.parquetæ–‡ä»¶.è¾“å‡ºå¯ä»¥éšä¾¿æŒ‡å®šä¸€ä¸ªäº‹å…ˆä¸å­˜åœ¨çš„æ–‡ä»¶å¤¹.)è¾“å‡ºParquetæ–‡ä»¶çš„å†…éƒ¨æ ¼å¼å¦‚ä¸‹:  


```
message hive_schema {
  optional int32 c1;
  optional boolean c2;
  optional double c3;
  optional binary c4;
  optional group c5 (LIST) {
    repeated group bag {
      optional int32 array_element;
    }
  }
  optional group c6 (MAP) {
    repeated group map (MAP_KEY_VALUE) {
      required int32 key;
      optional binary value;
    }
  }
  ...
}
```

å¯ä»¥çœ‹åˆ°å’Œparquet-tools schemaçš„è¾“å‡ºæ˜¯ä¸€æ ·çš„ç»“æœ.  

è¾“å‡ºæ–‡ä»¶å¤¹åŒ…æ‹¬äº†:

```
-rw-r--r--  1 zhengqh  staff     0B  6 23 09:53 _SUCCESS
-rw-r--r--  1 zhengqh  staff   531B  6 23 09:53 _common_metadata
-rw-r--r--  1 zhengqh  staff   2.2K  6 23 09:53 _metadata
-rw-r--r--  1 zhengqh  staff   2.8K  6 23 09:53 part-m-00000.parquet
```

**é—®é¢˜1: è¿è¡Œ```TestReadParquet.java```æ—¶æŠ¥é”™ç©ºæŒ‡é’ˆå¼‚å¸¸:**

```
java.lang.Exception: java.lang.NullPointerException
at org.apache.hadoop.mapred.LocalJobRunner$Job.runTasks(LocalJobRunner.java:462)
at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:522)
Caused by: java.lang.NullPointerException
at com.zqh.parquet.TestReadParquet$RecordSchema.<init>(TestReadParquet.java:46)
at com.zqh.parquet.TestReadParquet$ReadRequestMap.map(TestReadParquet.java:80)
at com.zqh.parquet.TestReadParquet$ReadRequestMap.map(TestReadParquet.java:70)
at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:145)
at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:787)
at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)
at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:243)
at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
at java.util.concurrent.FutureTask.run(FutureTask.java:266)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
at java.lang.Thread.run(Thread.java:745)
 INFO - Job job_local1516732386_0001 failed with state FAILED due to: NA
 INFO - Counters: 0
```
åŸå› åˆ†æ: å› ä¸ºä½¿ç”¨çš„æ˜¯hiveå†…åµŒçš„parquet(åœ¨hive-execä¸‹)

```
import parquet.Log;
import parquet.example.data.Group;
import parquet.hadoop.ParquetInputSplit;
import parquet.hadoop.example.ExampleInputFormat;
```

è€ŒParquetInputSplit.getFileSchame()ä¸­çš„å®ç°ç›´æ¥è¿”å›null,å¯¼è‡´ä¸Šé¢çš„ç©ºæŒ‡é’ˆ


```
    /** @deprecated */
    @Deprecated
    public String getFileSchema() {
        return null;
    }

```

**é—®é¢˜2: æ¢æˆapacheç‰ˆæœ¬çš„parquet**

```
import org.apache.parquet.Log;
import org.apache.parquet.example.data.Group;
import org.apache.parquet.hadoop.ParquetInputSplit;
import org.apache.parquet.hadoop.example.ExampleInputFormat;
```

åŸå› ä»ç„¶æ˜¯apacheç‰ˆæœ¬çš„å®ç°ä¸­ä¹Ÿæ˜¯ç›´æ¥æŠ›å‡ºäº†ä¸€ä¸ªå¼‚å¸¸:  

```
java.lang.Exception: java.lang.UnsupportedOperationException: Splits no longer have the file schema, see PARQUET-234
	at org.apache.hadoop.mapred.LocalJobRunner$Job.runTasks(LocalJobRunner.java:462)
	at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:522)
Caused by: java.lang.UnsupportedOperationException: Splits no longer have the file schema, see PARQUET-234
	at org.apache.parquet.hadoop.ParquetInputSplit.getFileSchema(ParquetInputSplit.java:187)
	at com.zqh.parquet.TestReadParquet$ReadRequestMap.map(TestReadParquet.java:82)
	at com.zqh.parquet.TestReadParquet$ReadRequestMap.map(TestReadParquet.java:74)
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:145)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:787)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)
	at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:243)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
 INFO - Job job_local1407158255_0001 failed with state FAILED due to: NA
 INFO - Counters: 0
```

æŸ¥çœ‹ParquetInputSlitçš„getFileSchema()æºç :é‡Œé¢æ ¹æœ¬å°±æ²¡æœ‰å®ç°æ–¹æ³•.

```
  /**
   * @return the file schema
   * @deprecated the file footer is no longer read before creating input splits
   */
  @Deprecated
  public String getFileSchema() {
    throw new UnsupportedOperationException(
        "Splits no longer have the file schema, see PARQUET-234");
  }
```

So, Don't Trust the Example! Run yourself!

### Parquet and Thrift dependency
æŒ‰ç…§æ–‡ç« ä¸­çš„è¯´æ³•: éœ€è¦thriftå’Œparquet-formatçš„jaråŒ…æ”¾åœ¨classpathä¸­:  
MapReduce needs thrift in its CLASSPATH and in libjars to access Parquet files. It also needs parquet-format in libjars. Perform the following setup before running MapReduce jobs that access Parquet data files:

```
if [ -e /opt/cloudera/parcels/CDH ] ; then
    CDH_BASE=/opt/cloudera/parcels/CDH
else
    CDH_BASE=/usr
fi
THRIFTJAR=`ls -l $CDH_BASE/lib/hive/lib/libthrift*jar | awk '{print $9}' | head -1`
export HADOOP_CLASSPATH=$HADOOP_CLASSPATH:$THRIFTJAR
export LIBJARS=`echo "$CLASSPATH" | awk 'BEGIN { RS = ":" } { print }' | grep parquet-format | tail -1`
export LIBJARS=$LIBJARS,$THRIFTJAR

hadoop jar my-parquet-mr.jar -libjars $LIBJARS
```

ç”±äºæˆ‘ä»¬ä¸æ˜¯æŒ‰ç…§CDH parcelsçš„æ–¹å¼å®‰è£…, æ‰€ä»¥jaråŒ…çš„è·¯å¾„è¦æ‰‹åŠ¨æŒ‡å®š: æ¯”å¦‚

```
/Users/zhengqh/Soft/cdh542/hive-1.1.0-cdh5.4.2/lib/libthrift-0.9.2.jar,
/Users/zhengqh/Soft/parquet-format-2.1.0-cdh5.4.2/target/parquet-format-2.1.0-cdh5.4.2.jar
```

### Install Thrift
å®‰è£…thrift: <https://github.com/apache/parquet-mr>  
å®˜æ–¹æ–‡æ¡£: <http://thrift.apache.org/docs/BuildingFromSource>

```
wget -nv http://archive.apache.org/dist/thrift/0.7.0/thrift-0.7.0.tar.gz
tar xzf thrift-0.7.0.tar.gz
cd thrift-0.7.0
chmod +x ./configure
chmod +x ./install-sh
./configure --disable-gen-erl --disable-gen-hs --without-ruby --without-haskell --without-erlang
sudo make install
```

å®‰è£…0.7.0ç‰ˆæœ¬çš„thriftä¼šå› ä¸ºphpç‰ˆæœ¬ä¸åŒ¹é…å¯¼è‡´å®‰è£…å¤±è´¥: <https://issues.apache.org/jira/browse/THRIFT-1602>  
è§£å†³æ–¹å¼æ˜¯åœ¨configureæ—¶ç›´æ¥æ·»åŠ : **--without-php**

```
/Users/zhengqh/Soft/thrift-0.7.0/lib/php/src/ext/thrift_protocol/php_thrift_protocol.cpp:95:8: error: unknown type name 'function_entry'
static function_entry thrift_protocol_functions[] = {
```

thriftä¾èµ–äº†boostå’Œlibevent: <http://thrift.apache.org/docs/install/os_x>

```
./configure --prefix=/usr/local/ --with-boost=/usr/local --with-libevent=/usr/local
```


#### macä¸‹å®‰è£…thrift
åœ¨macä¸‹å®‰è£…thriftå…¶å®ä¸€å¥è¯çš„äº‹å„¿,ä½†æ˜¯å¦‚æœä¹‹å‰ç”¨æºç è£…è¿‡äº†,ä¼šæŠ¥é”™:  

```
âœ brew install thrift

Error: The `brew link` step did not complete successfully
The formula built, but is not symlinked into /usr/local
Could not symlink bin/thrift
Target /usr/local/bin/thrift
already exists. You may want to remove it:
  rm '/usr/local/bin/thrift'

To force the link and overwrite all conflicting files:
  brew link --overwrite thrift

To list all files that would be deleted:
  brew link --overwrite --dry-run thrift

Possible conflicting files are:
/usr/local/bin/thrift
==> Summary
ğŸº  /usr/local/Cellar/thrift/0.9.2: 90 files, 5.4M
```

è®¾ç½®æ–°çš„è½¯é“¾æ¥:

```
âœ rm /usr/local/bin/thrift
âœ ln -s /usr/local/Cellar/thrift/0.9.2/bin/thrift /usr/local/bin/
âœ thrift -version
```

å®‰è£…æˆåŠŸå, æŸ¥çœ‹thriftç‰ˆæœ¬:  

```
thrift -version
Thrift version 0.9.2
```


### å‚è€ƒæ–‡æ¡£  
[How-to: Use Parquet with Impala, Hive, Pig, and MapReduce][1]  
[Understanding how Parquet integrates with Avro, Thrift and Protocol Buffers][2]

[1]: http://blog.cloudera.com/blog/2014/03/how-to-use-parquet-with-impala-hive-pig-mapreduce/
[2]: http://grepalex.com/2014/05/13/parquet-file-format-and-object-model/
